[How to self-host and hyperscale AI with Nvidia NIM](https://www.youtube.com/@Fireship)
#doit 
#ai 

[build.nvidia.com](build.nvidia.com)

running inference with models needs a bunch of gpu and memory

scaling it is fucking expensive

Nvidia currently have H100 gpus

they have a playground to test shit

saves you a bunch of time and you can use it IN BROWSER running in VS code

it is also containerized so you can do it everywhere

it also has Kubernetes and it's good to check so your models don't crash

using NIM as an augment to your abilities allows you to greatly shorten the development/deployment/maintenance 

in short, this is very fast, already optimized for speed and safety